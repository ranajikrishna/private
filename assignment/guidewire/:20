
import sys
import pandas as pd
import numpy as np
import multiprocessing as mp
from joblib import delayed, Parallel
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when
import pyspark.sql.functions as func 
from pyspark.sql.functions import *
from pyspark.sql import types as T
from pyspark import SparkContext
import h5py
import pickle as pk
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder

from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o.grid.grid_search import H2OGridSearch


import pdb

def gbm_model(df):
    h2o.init()
    h2o.init(nthreads=-1, strict_version_check=True)
    train, valid, test = df.split_frame(\
            ratios=[0.6, 0.3], \
            seed = 1234,\
            destination_frames=['train.hex','valid.hex','test.hex']
            )

    pdb.set_trace()
    # Establish Baseline performance.
    # We only provide the required parameters, everything else is default.
    gbm = H2OGradientBoostingEstimator()
    gbm.train(x=predictors, y=response, training_frame=train)
    pdb.set_trace()
    return 


def variable_encoding(df):

    # Columns that have `object` data types (`dtypes`)
    cols = df.columns[df.dtypes==object]
    # Convert `pre_harp` to 'N' and 'Y' 
    df[df['pre_harp'] != 'N'] = 'Y'
    # Drop loan sequence number and pre-harp
    cols=cols.drop(['loan_seq'])
    # Columns that are not ordinal. These can be encoded into `Labels`
    col_label = ['prp_state','seller_name','serv_name']

    for col in col_label:
        df[col] = df[col].astype('category').cat.codes

    # Columns that are ordinal: these will be one-hot-encoded
    col_one_hot = set(cols) - set(col_label)

    #creating instance of one-hot-encoder
    encoder = OneHotEncoder(handle_unknown='ignore')

    for col in col_one_hot:
        # Perform one-hot encoding on `col` column 
        encd_df = pd.DataFrame(encoder.fit_transform(df[[col]]).toarray())
        # Rename the column to `col_n`
        encd_df = encd_df.add_prefix(col + '_')
        # Join the new variable to the dataframe
        df.reset_index(inplace=True,drop=True)
        df = df.join(encd_df)
        df.drop(col,axis=1,inplace=True)
    
    df.set_index('loan_seq',inplace=True,drop=True)
    return df 


def handle_missing_data(df):
    '''
    Function to examine columns with empty strings.
    '''
    is_emp = ([])
    for col in df.columns:
        if df[col].isna:
            is_emp.append(col)

    # Handle missing MSA values.
    df.msa.fillna(99999,inplace=True)

    # Handle super conforming flag
    df.super.fillna('N',inplace=True)

    # Handle pre-HARP loan
    # Fill empty data with N (Not a pre-HARP)
    df.pre_harp.fillna('N',inplace=True)
    # Variable to identify: N-Not a pre-HARP, A-ARM, F-FRM
    df['pre_harp_status'] = df.pre_harp.str[0]

    # Handle HARP indicator
    df.hard_ind.fillna('N', inplace=True)

    # Drop loans that do not have monthly performance data
    df = df[df['del_def_bin'].notna()]

    # Replace NaN in zero balance removal upb by 0.
    df.zero_bal_upb.fillna(0,inplace=True)

    return df 


def examine_loan_age(df):
    '''
    Function to identify columns in "monthly performance data", after retaining
    only the rows with `loan age = '000'`, have no data. These columns are 
    excluded from the final data set
    '''
    df = df.replace('', 'null')
    for col in df.columns:
        df.agg(countDistinct(col)).show()
    return

def get_del_def_status(x):
    del_status = set(['3'])
    return 1 - int(set(x).intersection(del_status)==set())

def get_zero_def_status(x):
    del_status = set(['03','06','09'])
    return 1 - int(set(x).intersection(del_status)==set())

def get_data():

    sc = SparkContext.getOrCreate()
    spark = SparkSession(sc)
    # ---- Load 2009 Q1 historical data ----
    filepath = '/Users/vashishtha/myGitCode/private/figure/historical_data_2009Q1/historical_data_2009Q1.txt'
    data_rdd = sc.textFile(filepath)
    columns = ['credit_score','first_pmt_date','first_time_buyer','mat_date','msa','mi',\
            'units','ocp_status','cltv','dti','upb','ltv','ori_int_rate','channel',\
            'ppm','amtr_type','prp_state','prp_type','post_code','loan_seq','loan_purpose',\
            'ori_loan_term','num_borrower','seller_name','serv_name','super','pre_harp',\
            'prog_ind','hard_ind','prp_val_method','io_ind']
    # Data: 2009 Q1 historical data
    data = data_rdd.map(lambda x: x.split('|')).toDF(columns)
    
    # ---- Load 2009 Q1 monthly performance data ----
    filepath = '/Users/vashishtha/myGitCode/private/figure/historical_data_2009Q1/historical_data_time_2009Q1.txt'
    time_rdd = sc.textFile(filepath)
    columns = ['loan_seq','mth_rep_prd','act_upb','loan_del_status','loan_age',\
            'rem_mth_mat','def_setl','mod_flag','zero_bal_cde','zero_bal_dte',\
            'int_rate','def_upb','ddlpi','mi_rec','net_sale','non_mi_rec','exp',\
            'lgl_cost','map_costs','tax_insr','misc_exp','act_loss','mod_cost',\
            'step_mod_flag','def_pmt_plan','eltv','zero_bal_upb','del_acd_int','del_dis',\
            'brw_asst_cde','cmm_cost','int_upb']
    time = time_rdd.map(lambda x: x.split('|')).toDF(columns)

    # Two ways of handling of missing `loan age = '000'`: 
    # [1] Remove loans that have missing `loan age = '000'`
    # [2] Get the next minimum loan age, which is `loan age = '001'`
    # Method [1]
#    time_origin = time.filter(time.loan_age=='000')
    # Method [2]
    time_min_loan_age = time.groupBy('loan_seq').agg(func.min('loan_age').\
                                                        alias('min_loan_age'))
    time_origin = time.join(time_min_loan_age, (time.loan_seq == \
            time_min_loan_age.loan_seq) & \
            (time.loan_age==time_min_loan_age.min_loan_age) , "inner").select(time["*"])
    # Examine columns of dataframe that have no data in them.
#    examine_loan_age(time_origin)

    # Compute default status by using `zero balance code`
    zero_bal_def = func.udf(get_zero_def_status,T.IntegerType())
    def_bal_cde = time.groupBy('loan_seq').agg(zero_bal_def(func.collect_list(\
                                    'zero_bal_cde')).alias('zero_def_bin'))
    # Compute default status by using `Current Loan Delinquency Status`
    del_stat_def = func.udf(get_del_def_status,T.IntegerType())
    def_del_stat = time.groupBy('loan_seq').agg(del_stat_def(func.collect_list(\
                                    'loan_del_status')).alias('del_def_bin'))

    universe = \
    data.join(time_origin,data.loan_seq==time_origin.loan_seq, how='left').join(def_bal_cde, \
        data.loan_seq==def_bal_cde.loan_seq, how='left').join(def_del_stat, \
        data.loan_seq==def_del_stat.loan_seq, how='left').select(data["*"], \
        time_origin["act_upb"],time_origin["rem_mth_mat"],time_origin["int_upb"], \
        time_origin["zero_bal_upb"],def_bal_cde["zero_def_bin"], \
        def_del_stat["del_def_bin"])
    # Save data in pickle format.
    universe_pd = universe.toPandas()
    universe_pd.to_csv('universe_all.csv')

    return 

def main():
    # Process and Examine data.
    # get_data()
    universe = pd.read_csv('universe_all.csv')
    # Examine columns with empty string
    universe_clean = handle_missing_data(universe)
    universe_clean.drop('Unnamed: 0',axis=1,inplace=True)
    data = variable_encoding(universe_clean)
    gbm_model(data)

    

    return 

if __name__ == '__main__':
    status = main()
    sys.exit()
